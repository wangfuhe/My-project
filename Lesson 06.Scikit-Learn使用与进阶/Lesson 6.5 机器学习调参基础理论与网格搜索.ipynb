{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6.5 机器学习调参基础理论与网格搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在上一小节执行完手动调参之后，接下来我们重点讨论关于机器学习调参的理论基础，并且介绍sklearn中调参的核心工具——GridSearchCV。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 科学计算模块\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 绘图模块\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 自定义模块\n",
    "from ML_basic_function import *\n",
    "\n",
    "# Scikit-Learn相关模块\n",
    "# 评估器类\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# 实用函数\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、机器学习调参理论基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在利用sklearn进行机器学习调参之前，我们先深入探讨一些和调参相关的机器学习基础理论。尽管我们都知道，调参其实就是去寻找一组最优参数，但最优参数中的“最优”如何定义？面对模型中的众多参数又该如何“寻找”？要回答这些问题，我们就必须补充更加完整的关于机器学习中参数和调参的理论知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.机器学习调参目标及基本方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先需要明确的一点，我们针对哪一类参数进行调参，以及围绕什么目的进行调参？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数与超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;根据此前对参数的划分，我们知道，影响机器学习建模结果的参数有两类，其一是参数，其二是超参数。其中参数的数值计算由一整套数学过程决定，在选定方法后，其计算过程基本不需要人工参与。因此我们经常说的模型调参，实际上是调整模型超参数。超参数种类繁多，而且无法通过一个严谨的数学流程给出最优解，因此需要人工参与进行调节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而在围绕具体的机器学习评估器进行调参时，其实就是在调整评估器实例化过程中所涉及到的那些超参数，例如此前进行逻辑回归参数解释时的超参数，当然，这也是我们为什么需要对评估器进行如此详细的超参数的解释的原因之一。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|参数|解释|\n",
    "|:--:|:--:|    \n",
    "|penalty|正则化项|   \n",
    "|dual|是否求解对偶问题*|\n",
    "|tol|迭代停止条件：两轮迭代损失值差值小于tol时，停止迭代|\n",
    "|C|经验风险和结构风险在损失函数中的权重|\n",
    "|fit_intercept|线性方程中是否包含截距项|\n",
    "|intercept_scaling|相当于此前讨论的特征最后一列全为1的列，当使用liblinear求解参数时用于捕获截距|\n",
    "|class_weight|各类样本权重*|\n",
    "|random_state|随机数种子|\n",
    "|solver|损失函数求解方法*|\n",
    "|max_iter|求解参数时最大迭代次数，迭代过程满足max_iter或tol其一即停止迭代|\n",
    "|multi_class|多分类问题时求解方法*|\n",
    "|verbose|是否输出任务进程|\n",
    "|warm_start|是否使用上次训练结果作为本次运行初始参数|\n",
    "|l1_ratio|当采用弹性网正则化时，$l1$正则项权重，就是损失函数中的$\\rho$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 超参数调整目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;那么紧接着的问题就是，超参数的调整目标是什么？是提升模型测试集的预测效果么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;无论是机器学习还是统计模型，只要是进行预测的模型，其实核心的建模目标都是为了更好的进行预测，也就是希望模型能够有更好的预测未来的能力，换而言之，就是希望模型能够有更强的泛化能力。而在Lesson 3中我们曾谈到，机器学习类算法的可信度来源则是训练集和测试集的划分理论，也就是机器学习会认为，只要能够在模拟真实情况的测试集上表现良好，模型就能够具备良好的泛化能力。也就是说，超参数调整的核心目的是为了提升模型的泛化能力，而测试集上的预测效果只是模型泛化能力的一个具体表现，并且相比与一次测试集上的运行结果，其实借助交叉验证，能够提供更有效、更可靠的模型泛化能力的证明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 交叉验证与评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果需要获得更可靠的模型泛化能力的证明，则需要进行交叉验证，通过多轮的验证，来获得模型的更为一般、同时也更为准确的运行结果。当然，我们还需要谨慎的选择一个合适的评估指标对其进行结果评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如何提升模型泛化能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果拥有了一个更加可信的、用于验证模型是否具有泛化能力的评估方式之后，那么接下来的问题就是，我们应该如何提升模型泛化能力呢？        \n",
    "&emsp;&emsp;当然，这其实是一个很大的问题，我们可以通过更好的选择模型（甚至是模型创新）、更好的特征工程、更好的模型训练等方法来提高模型泛化能力，而此处我们将要介绍的，是围绕某个具体的模型、通过更好的选择模型中的超参数，来提高模型的泛化能力。不过正如此前所说，超参数无法通过一个严谨的数学流程给出最优解，因此超参数的选择其实是经验+一定范围内枚举（也就是网格搜索）的方法来决定的。这个过程虽然看起来不是那么的cooooool，但确实目前机器学习超参数选择的通用方式，并且当我们深入进行了解之后就会发现，尽管是经验+枚举，但经验的积累和枚举技术的掌握，其实也是算法工程师建模水平的重要证明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.基于网格搜索的超参数的调整方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在了解机器学习中调参的基础理论之后，接下来我们考虑一个更加具体的调参流程。实际上，尽管对于机器学习来说超参数众多，但能够对模型的建模结果产生决定性影响的超参数却不多，对于大多数超参数，我们都主要采用“经验结合实际”的方式来决定超参数的取值，如数据集划分比例、交叉验证的折数等等，而对于一些如正则化系数、特征衍生阶数等，则需要采用一个流程来对其进行调节。而这个流程，一般来说就是进行搜索与枚举，或者也被称为网格搜索（gridsearch）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;所谓搜索与枚举，指的是将备选的参数一一列出，多个不同参数的不同取值最终将组成一个参数空间（parameter space），在这个参数空间中选取不同的值带入模型进行训练，最终选取一组最优的值作为模型的最终超参数，当然，正如前面所讨论的，此处“最优”的超参数，应该是那些尽可能让模型泛化能力更好的参数。当然，在这个过程中，有两个核心问题需要注意，其一是参数空间的构成，其二是选取能够代表模型泛化能力的评估指标。接下来我们对其进行逐个讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 参数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数空间的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;所谓参数空间，其实就是我们挑选出来的、接下来需要通过枚举和搜索来进行数值确定的参数取值范围所构成的空间。例如对于逻辑回归模型来说，如果选择penalty参数和C来进行搜索调参，则这两个参数就是参数空间的不同维度，而这两个参数的不同取值就是这个参数空间中的一系列点，例如(penalty='l1', C=1)、(penalty='l1', C=0.9)、(penalty='l2', C=0.8)等等，就是这个参数空间内的一系列点，接下来我们就需要从中挑选组一个最优组合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数空间构造思路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;那么我们需要带入那些参数去构造这个参数空间呢？也就是我们需要选择那些参数进行调参呢？切记，调参的目的是为了提升模型的泛化能力，而保证泛化能力的核心是同时控制模型的经验风险和结构风险（既不让模型过拟合也不让模型前拟合），因此，对于逻辑回归来说，我们需要同时带入能够让模型拟合度增加、同时又能抑制模型过拟合倾向的参数来构造参数空间，即需要带入特征衍生的相关参数、以及正则化的相关参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 一个建模流程中的特征衍生的相关参数，也是可以带入同一个参数空间进行搜索的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 交叉验证与评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;实际的超参数的搜索过程和我们上面讨论的模型结构风险一节中的参数选取过程略有不同，此前我们的过程是：先在训练集中训练模型，然后计算训练误差和泛化误差，通过二者误差的比较来观察模型是过拟合还是欠拟合（即评估模型泛化能力），然后再决定这些超参数应该如何调整。而在一个更加严谨的过程中，我们需要将上述“通过对比训练误差和测试误差的差异，来判断过拟合还是欠拟合”的这个偏向主观的过程变成一个更加客观的过程，即我们需要找到一个能够基于目前模型建模结果的、能代表模型泛化能力的评估指标，这即是模型建模流程更加严谨的需要，同时也是让测试集回归其本来定位的需要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 评估指标选取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而这个评估指标，对于分类模型来说，一般来说就是ROC-AUC或F1-Score，并且是基于交叉验证之后的指标。我们通常会选取ROC-AUC或F1-Score，其实也是因为这两个指标的敏感度要强于准确率（详见Lesson 5中的讨论），并且如果需要重点识别模型识别1类的能力，则可考虑F1-Score，其他时候更推荐使用ROC-AUC。      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 交叉验证过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而为何要进行交叉验证，则主要原因是超参数的调整也需要同时兼顾模型的结构风险和经验风险，而能够表示模型结构风险的，就是不带入模型训练、但是能够对模型建模结果进行评估并且指导模型进行调整的验证集上的评估结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;上述过程可以具体表示成如下步骤：      \n",
    "- 在训练集中进行验证集划分（几折待定）；\n",
    "- 带入训练集进行建模、带入验证集进行验证，并输出验证集上的模型评估指标；      \n",
    "- 计算多组验证集上的评估指标的均值，作为该超参数下模型最终表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;因此，在大多数情况下，网格搜索（gridsearch）都是和交叉验证（CV）同时出现的，这也是为什么sklearn中执行网格搜索的类名称为GridSearchCV的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;另外需要强调的一点是，由于交叉验证的存在，此时测试集的作用就变成了验证网格搜索是否有效，而非去验证模型是否有效（此时模型是否有效由验证集来验证）。由于此时我们提交给测试集进行测试的，都是经过交叉验证挑选出来的最好的一组参数、或者说至少是在验证集上效果不错的参数（往往也是评估指标比较高的参数），而此时如果模型在测试集上运行效果不好、或者说在测试集上评估指标表现不佳，则说明模型仍然还是过拟合，之前执行的网格搜索过程并没有很好的控制住模型的结构风险，据此我们需要调整此前的调参策略，如调整参数空间、或者更改交叉验证策略等。        \n",
    "&emsp;&emsp;当然，如果是对网格搜索的过程比较自信，也可以不划分测试集，直接带入全部数据进行模型训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、基于Scikit-Learn的网格搜索调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在了解机器学习调参基础理论之后，接下来我们来借助sklearn中的相关工具，来执行更加高效的调参工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.sklearn中网格搜索的基本说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;由于网格搜索确定超参数的过程实际上帮助进行模型筛选，因此我们可以在sklearn的`model_selection`模块查找相关内容。要学习sklearn中的网格搜索相关功能，最好还是从查阅官网的说明文档开始，我们可以在sklearn的User Guide的3.2节中我们能看到关于网格搜索的相关内容。首先介绍官网给出的相关说明："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://tva1.sinaimg.cn/large/008i3skNly1gsbvgh9n81j31h40pqdzj.jpg\" alt=\"1\" style=\"zoom:30%;\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;该说明文档开宗明义的介绍了网格搜索根本目的是为了调整超参数（Hyper-parameters），也就是评估器（estimators）中的参数，每个评估器中的参数可以通过.get_params()的方法来查看，并且建议配合交叉验证来执行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;同时，该说明文档重点指出了网格搜索中的核心要素，分别是：评估器、参数空间、搜索策略、交叉验证以及评估指标。其中参数空间、交叉验证以及评估指标我们都在此前介绍过了，而根据下文的介绍，sklearn中实际上是集成了两种不同的进行参数搜索的方法，分别是`GridSearchCV`和`RandomizedSearchCV`："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://tva1.sinaimg.cn/large/008i3skNly1gsbvnsmyv6j31gm0fsk9q.jpg\" alt=\"1\" style=\"zoom:30%;\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;尽管都是进行网格搜索，但两种方法还是各有不同，GridSearchCV会尝试参数空间内的所有组合，而RandomizedSearchCV则会先进行采样再来进行搜索，即对某个参数空间的某个随机子集进行搜索。并且上文重点强调，这两种方法都支持先两两比对、然后逐层筛选的方法来进行参数筛选，即HalvingGridSearchCV和HalvingRandomSearchCV方法。注意，这是sklearn最新版、也就是0.24版才支持的功能，该功能的出现也是0.24版最大的改动之一，而该功能的加入，也将进一步减少网格搜索所需计算资源、加快网格搜索的速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 由于目前sklearn中最通用的版本还是0.23版，因此0.24版中的提供的网格筛选的新功能在正课内容暂时不做介绍，后续将以加餐形式补充进行讲解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，说明文档中也再次强调，由于sklearn的评估器中集成了非常多的参数，而并非所有参数都对最终建模结果有显著影响，因此为了不增加网格搜索过程计算量，推荐谨慎的构造参数空间，部分参数仍然以默认参数为主。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在介绍完基本说明文档后，接下来我们尝试调用sklearn中集成的相关方法来进行建模试验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.sklearn中GridSearchCV的参数解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们详细介绍GridSearchCV的相关参数，我们知道该方法的搜索策略是“全搜索”，即对参数空间内的所有参数进行搜索，该方法在model_selection模块下，同样也是以评估器形式存在，我们可以通过如下方式进行导入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不难发现该评估器的参数主体就是此前介绍的评估器、参数空间、交叉验证以及评估指标，我们对该评估器的完整参数进行解释："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0miid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2*n_jobs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0merror_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Exhaustive search over specified parameter values for an estimator.\n",
       "\n",
       "Important members are fit, predict.\n",
       "\n",
       "GridSearchCV implements a \"fit\" and a \"score\" method.\n",
       "It also implements \"predict\", \"predict_proba\", \"decision_function\",\n",
       "\"transform\" and \"inverse_transform\" if they are implemented in the\n",
       "estimator used.\n",
       "\n",
       "The parameters of the estimator used to apply these methods are optimized\n",
       "by cross-validated grid-search over a parameter grid.\n",
       "\n",
       "Read more in the :ref:`User Guide <grid_search>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "estimator : estimator object.\n",
       "    This is assumed to implement the scikit-learn estimator interface.\n",
       "    Either estimator needs to provide a ``score`` function,\n",
       "    or ``scoring`` must be passed.\n",
       "\n",
       "param_grid : dict or list of dictionaries\n",
       "    Dictionary with parameters names (`str`) as keys and lists of\n",
       "    parameter settings to try as values, or a list of such\n",
       "    dictionaries, in which case the grids spanned by each dictionary\n",
       "    in the list are explored. This enables searching over any sequence\n",
       "    of parameter settings.\n",
       "\n",
       "scoring : str, callable, list/tuple or dict, default=None\n",
       "    A single str (see :ref:`scoring_parameter`) or a callable\n",
       "    (see :ref:`scoring`) to evaluate the predictions on the test set.\n",
       "\n",
       "    For evaluating multiple metrics, either give a list of (unique) strings\n",
       "    or a dict with names as keys and callables as values.\n",
       "\n",
       "    NOTE that when using custom scorers, each scorer should return a single\n",
       "    value. Metric functions returning a list/array of values can be wrapped\n",
       "    into multiple scorers that return one value each.\n",
       "\n",
       "    See :ref:`multimetric_grid_search` for an example.\n",
       "\n",
       "    If None, the estimator's score method is used.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    Number of jobs to run in parallel.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "\n",
       "    .. versionchanged:: v0.20\n",
       "       `n_jobs` default changed from 1 to None\n",
       "\n",
       "pre_dispatch : int, or str, default=n_jobs\n",
       "    Controls the number of jobs that get dispatched during parallel\n",
       "    execution. Reducing this number can be useful to avoid an\n",
       "    explosion of memory consumption when more jobs get dispatched\n",
       "    than CPUs can process. This parameter can be:\n",
       "\n",
       "        - None, in which case all the jobs are immediately\n",
       "          created and spawned. Use this for lightweight and\n",
       "          fast-running jobs, to avoid delays due to on-demand\n",
       "          spawning of the jobs\n",
       "\n",
       "        - An int, giving the exact number of total jobs that are\n",
       "          spawned\n",
       "\n",
       "        - A str, giving an expression as a function of n_jobs,\n",
       "          as in '2*n_jobs'\n",
       "\n",
       "iid : bool, default=False\n",
       "    If True, return the average score across folds, weighted by the number\n",
       "    of samples in each test set. In this case, the data is assumed to be\n",
       "    identically distributed across the folds, and the loss minimized is\n",
       "    the total loss per sample, and not the mean loss across the folds.\n",
       "\n",
       "    .. deprecated:: 0.22\n",
       "        Parameter ``iid`` is deprecated in 0.22 and will be removed in 0.24\n",
       "\n",
       "cv : int, cross-validation generator or an iterable, default=None\n",
       "    Determines the cross-validation splitting strategy.\n",
       "    Possible inputs for cv are:\n",
       "\n",
       "    - None, to use the default 5-fold cross validation,\n",
       "    - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
       "    - :term:`CV splitter`,\n",
       "    - An iterable yielding (train, test) splits as arrays of indices.\n",
       "\n",
       "    For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
       "    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
       "    other cases, :class:`KFold` is used.\n",
       "\n",
       "    Refer :ref:`User Guide <cross_validation>` for the various\n",
       "    cross-validation strategies that can be used here.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "        ``cv`` default value if None changed from 3-fold to 5-fold.\n",
       "\n",
       "refit : bool, str, or callable, default=True\n",
       "    Refit an estimator using the best found parameters on the whole\n",
       "    dataset.\n",
       "\n",
       "    For multiple metric evaluation, this needs to be a `str` denoting the\n",
       "    scorer that would be used to find the best parameters for refitting\n",
       "    the estimator at the end.\n",
       "\n",
       "    Where there are considerations other than maximum score in\n",
       "    choosing a best estimator, ``refit`` can be set to a function which\n",
       "    returns the selected ``best_index_`` given ``cv_results_``. In that\n",
       "    case, the ``best_estimator_`` and ``best_params_`` will be set\n",
       "    according to the returned ``best_index_`` while the ``best_score_``\n",
       "    attribute will not be available.\n",
       "\n",
       "    The refitted estimator is made available at the ``best_estimator_``\n",
       "    attribute and permits using ``predict`` directly on this\n",
       "    ``GridSearchCV`` instance.\n",
       "\n",
       "    Also for multiple metric evaluation, the attributes ``best_index_``,\n",
       "    ``best_score_`` and ``best_params_`` will only be available if\n",
       "    ``refit`` is set and all of them will be determined w.r.t this specific\n",
       "    scorer.\n",
       "\n",
       "    See ``scoring`` parameter to know more about multiple metric\n",
       "    evaluation.\n",
       "\n",
       "    .. versionchanged:: 0.20\n",
       "        Support for callable added.\n",
       "\n",
       "verbose : integer\n",
       "    Controls the verbosity: the higher, the more messages.\n",
       "\n",
       "error_score : 'raise' or numeric, default=np.nan\n",
       "    Value to assign to the score if an error occurs in estimator fitting.\n",
       "    If set to 'raise', the error is raised. If a numeric value is given,\n",
       "    FitFailedWarning is raised. This parameter does not affect the refit\n",
       "    step, which will always raise the error.\n",
       "\n",
       "return_train_score : bool, default=False\n",
       "    If ``False``, the ``cv_results_`` attribute will not include training\n",
       "    scores.\n",
       "    Computing training scores is used to get insights on how different\n",
       "    parameter settings impact the overfitting/underfitting trade-off.\n",
       "    However computing the scores on the training set can be computationally\n",
       "    expensive and is not strictly required to select the parameters that\n",
       "    yield the best generalization performance.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "    .. versionchanged:: 0.21\n",
       "        Default value was changed from ``True`` to ``False``\n",
       "\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn import svm, datasets\n",
       ">>> from sklearn.model_selection import GridSearchCV\n",
       ">>> iris = datasets.load_iris()\n",
       ">>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
       ">>> svc = svm.SVC()\n",
       ">>> clf = GridSearchCV(svc, parameters)\n",
       ">>> clf.fit(iris.data, iris.target)\n",
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n",
       ">>> sorted(clf.cv_results_.keys())\n",
       "['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n",
       " 'param_C', 'param_kernel', 'params',...\n",
       " 'rank_test_score', 'split0_test_score',...\n",
       " 'split2_test_score', ...\n",
       " 'std_fit_time', 'std_score_time', 'std_test_score']\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "cv_results_ : dict of numpy (masked) ndarrays\n",
       "    A dict with keys as column headers and values as columns, that can be\n",
       "    imported into a pandas ``DataFrame``.\n",
       "\n",
       "    For instance the below given table\n",
       "\n",
       "    +------------+-----------+------------+-----------------+---+---------+\n",
       "    |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n",
       "    +============+===========+============+=================+===+=========+\n",
       "    |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n",
       "    +------------+-----------+------------+-----------------+---+---------+\n",
       "    |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n",
       "    +------------+-----------+------------+-----------------+---+---------+\n",
       "    |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n",
       "    +------------+-----------+------------+-----------------+---+---------+\n",
       "    |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n",
       "    +------------+-----------+------------+-----------------+---+---------+\n",
       "\n",
       "    will be represented by a ``cv_results_`` dict of::\n",
       "\n",
       "        {\n",
       "        'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n",
       "                                     mask = [False False False False]...)\n",
       "        'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n",
       "                                    mask = [ True  True False False]...),\n",
       "        'param_degree': masked_array(data = [2.0 3.0 -- --],\n",
       "                                     mask = [False False  True  True]...),\n",
       "        'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n",
       "        'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n",
       "        'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n",
       "        'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n",
       "        'rank_test_score'    : [2, 4, 3, 1],\n",
       "        'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n",
       "        'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n",
       "        'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n",
       "        'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n",
       "        'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n",
       "        'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n",
       "        'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n",
       "        'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n",
       "        'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
       "        }\n",
       "\n",
       "    NOTE\n",
       "\n",
       "    The key ``'params'`` is used to store a list of parameter\n",
       "    settings dicts for all the parameter candidates.\n",
       "\n",
       "    The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
       "    ``std_score_time`` are all in seconds.\n",
       "\n",
       "    For multi-metric evaluation, the scores for all the scorers are\n",
       "    available in the ``cv_results_`` dict at the keys ending with that\n",
       "    scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
       "    above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
       "\n",
       "best_estimator_ : estimator\n",
       "    Estimator that was chosen by the search, i.e. estimator\n",
       "    which gave highest score (or smallest loss if specified)\n",
       "    on the left out data. Not available if ``refit=False``.\n",
       "\n",
       "    See ``refit`` parameter for more information on allowed values.\n",
       "\n",
       "best_score_ : float\n",
       "    Mean cross-validated score of the best_estimator\n",
       "\n",
       "    For multi-metric evaluation, this is present only if ``refit`` is\n",
       "    specified.\n",
       "\n",
       "    This attribute is not available if ``refit`` is a function.\n",
       "\n",
       "best_params_ : dict\n",
       "    Parameter setting that gave the best results on the hold out data.\n",
       "\n",
       "    For multi-metric evaluation, this is present only if ``refit`` is\n",
       "    specified.\n",
       "\n",
       "best_index_ : int\n",
       "    The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
       "    candidate parameter setting.\n",
       "\n",
       "    The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
       "    the parameter setting for the best model, that gives the highest\n",
       "    mean score (``search.best_score_``).\n",
       "\n",
       "    For multi-metric evaluation, this is present only if ``refit`` is\n",
       "    specified.\n",
       "\n",
       "scorer_ : function or a dict\n",
       "    Scorer function used on the held out data to choose the best\n",
       "    parameters for the model.\n",
       "\n",
       "    For multi-metric evaluation, this attribute holds the validated\n",
       "    ``scoring`` dict which maps the scorer key to the scorer callable.\n",
       "\n",
       "n_splits_ : int\n",
       "    The number of cross-validation splits (folds/iterations).\n",
       "\n",
       "refit_time_ : float\n",
       "    Seconds used for refitting the best model on the whole dataset.\n",
       "\n",
       "    This is present only if ``refit`` is not False.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The parameters selected are those that maximize the score of the left out\n",
       "data, unless an explicit score is passed in which case it is used instead.\n",
       "\n",
       "If `n_jobs` was set to a value higher than one, the data is copied for each\n",
       "point in the grid (and not `n_jobs` times). This is done for efficiency\n",
       "reasons if individual jobs take very little time, but may raise errors if\n",
       "the dataset is large and not enough memory is available.  A workaround in\n",
       "this case is to set `pre_dispatch`. Then, the memory is copied only\n",
       "`pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
       "n_jobs`.\n",
       "\n",
       "See Also\n",
       "---------\n",
       ":class:`ParameterGrid`:\n",
       "    generates all the combinations of a hyperparameter grid.\n",
       "\n",
       ":func:`sklearn.model_selection.train_test_split`:\n",
       "    utility function to split the data into a development set usable\n",
       "    for fitting a GridSearchCV instance and an evaluation set for\n",
       "    its final evaluation.\n",
       "\n",
       ":func:`sklearn.metrics.make_scorer`:\n",
       "    Make a scorer from a performance metric or loss function.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GridSearchCV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Name|Description|      \n",
    "|:--:|:--:|      \n",
    "|estimator|调参对象，某评估器|      \n",
    "|param_grid|参数空间，可以是字典或者字典构成的列表，稍后介绍参数空间的创建方法|\t      \n",
    "|scoring|评估指标，支持同时输出多个参数|\n",
    "|n_jobs|设置工作时参与计算的CPU核数|\n",
    "|iid|交叉验证时各折数据是否独立，该参数已在0.22版中停用，将在0.24版中弃用，此处不做介绍|\n",
    "|refit|挑选评估指标和最佳参数，在完整数据集上进行训练|\n",
    "|cv|交叉验证的折数|\n",
    "|verbose|输出工作日志形式|\n",
    "|pre_dispatch|多任务并行时任务划分数量|\n",
    "|error_score|当网格搜索报错时返回结果，选择'raise'时将直接报错并中断训练过程，其他情况会显示警告信息后继续完成训练|\n",
    "|return_train_score|在交叉验证中是否显示训练集中参数得分|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;整体来看，上面的主要参数分为三类，分别是核心参数、评估参数和性能参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 核心参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;所谓性能参数，也就是涉及评估器训练（fit）的最核心参数，也就是estimator参数和param_grid参数，同时也是实例化评估器过程中最重要的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 评估参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;所谓评估参数，指的是涉及到不同参数训练结果评估过程方式的参数，主要是scoring、refit和cv三个参数。当然这三个参数都不是必要参数，但这三个参数却是直接决定模型结果评估过程、并且对最终模型参数选择和模型泛化能力提升直观重要的三个参数。这三个参数各自都有一个默认值，我们先解释在默认值情况下这三个参数的运作方式，然后在进阶应用阶段讨论如何对这三个参数进行修改。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先是关于scoring参数的选取，scoring表示选取哪一项评估指标来对模型结果进行评估。而根据参数说明文档我们知道，在默认情况下scoring的评估指标就是评估器的`.score`方法默认的评估指标，对于逻辑回归来说也就是准确率。也就是说在默认情况下如果是围绕逻辑回归进行网格搜索，则默认评估指标是准确率。此外，scoring参数还支持直接输入可调用对象（评估函数）、代表评估函数运行方式的字符串、字典或者list。而refit参数则表示选择一个用于评估最佳模型的评估指标，然后在最佳参数的情况下整个训练集上进行对应评估指标的计算。而cv则是关于交叉验证的相关参数，默认情况下进行5折交叉验证，并同时支持自定义折数的交叉验证、输入交叉验证评估器的交叉验证、以及根据指定方法进行交叉验证等方法。当然此组参数有非常多的设计方法，我们将在进阶应用阶段进行进一步的详解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 性能参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;第三组则是关于网格搜索执行性能相关的性能参数，主要包括n_jobs和pre_dispatch参数两个，用于规定调用的核心数和一个任务按照何种方式进行并行运算。在网格搜索中，由于无需根据此前结果来确定后续计算方法，所以可以并行计算。在默认情况下并行任务的划分数量和n_jobs相同。当然，这组参数的合理设置能够一定程度提高模型网格搜索效率，但如果需要大幅提高执行速度，建议使用RandomizedSearchCV、或者使用Halving方法来进行加速。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.sklearn中GridSearchCV的使用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在了解了GridSearchCV的基本方法之后，接下来我们以逻辑回归在鸢尾花数据集上建模为例，来尝试使用GridSearchCV方法进行网格调参，并同时介绍网格搜索的一般流程："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 GridSearchCV评估器训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1.创建评估器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先我们还是需要实例化一个评估器，这里可以是一个模型、也可以是一个机器学习流，网格搜索都可以对其进行调参。此处我们先从简单入手，尝试实例化逻辑回归模型并对其进行调参。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据导入\n",
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=int(1e6), solver='saga')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 此处将solver设置成saga，也是为了方便后续同时比较l1正则化和l2正则化时无需更换求解器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 1000000,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'saga',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 2.创建参数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，我们就需要挑选评估器中的超参数构造参数空间，需要注意的是，我们需要挑选能够控制模型拟合度的超参数来进行参数空间的构造，例如挑选类似verbose、n_jobs等此类参数构造参数是毫无意义的。此处我们挑选penalty和C这两个参数来进行参数空间的构造。参数空间首先可以是一个字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_simple = {'penalty': ['l1', 'l2'],\n",
    "                     'C': [1, 0.5, 0.1, 0.05, 0.01]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，字典的Key用参数的字符串来代表不同的参数，对应的Value则用列表来表示对应参数不同的取值范围。也就是字典的Key是参数空间的维度，而Value则是不同纬度上可选的取值。而后续的网格搜索则是在上述参数的不同组合中挑选出一组最优的参数取值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，由于如此构造方法，此处自然会衍生出一个新的问题，那就是如果某个维度的参数取值对应一组新的参数，应该如何处理？例如，对于逻辑回归来说，如果penalty参数中选择弹性网参数，则会衍生出一个新的参数l1_ratio，如果我们还想考虑penalty参数选取elasticnet参数，并且同时评估l1_ratio取不同值时模型效果，则无法将上述参数封装在一个参数空间内，因为当penalty取其他值时l1_ratio并不存在。为了解决这个问题，我们可以创造多个参数空间（字典），然后将其封装在一个列表中，而该列表则表示多个参数空间的集成。例如上述问题可以进行如下表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ra = [\n",
    "    {'penalty': ['l1', 'l2'], 'C': [1, 0.5, 0.1, 0.05, 0.01]}, \n",
    "    {'penalty': ['elasticnet'], 'C': [1, 0.5, 0.1, 0.05, 0.01], 'l1_ratio': [0.3, 0.6, 0.9]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即可表示网格搜索在l1+1、l1+0.5...空间与elasticnet+1+0.3、elasticnet+1+0.6...空间同时进行搜索。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 3.实例化网格搜索评估器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;和sklearn中其他所有评估器一样，网格搜索的评估器的使用也是先实例化然后进行对其进行训练。此处先实例化一个简单的网格搜索评估器，需要输入此前设置的评估器和参数空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(estimator=clf,\n",
    "                      param_grid=param_grid_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 4.训练网格搜索评估器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;同样，我们通过fit方法即可完成评估器的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(max_iter=1000000, solver='saga'),\n",
       "             param_grid={'C': [1, 0.5, 0.1, 0.05, 0.01],\n",
       "                         'penalty': ['l1', 'l2']})"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;需要知道的是，所谓的训练网格搜索评估器，本质上是在挑选不同的参数组合进行逻辑回归模型训练，而训练完成后相关结果都保存在search对象的属性中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 GridSearchCV评估器结果查看"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此处我们先介绍关于网格搜索类的所有属性和方法，再来查看挑选其中重要属性的结果进行解读。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Name|Description|      \n",
    "|:--:|:--:| \n",
    "|cv_results_|交叉验证过程中的重要结果| \n",
    "|best_estimator_|最终挑选出的最优|      \n",
    "|best_score_|在最优参数情况下，训练集的交叉验证的平均得分|\t      \n",
    "|best_params_|最优参数组合|\n",
    "|best_index_|CV过程会对所有参数组合标号，该参数表示最优参数组合的标号|\n",
    "|scorer|在最优参数下，计算模型得分的方法|\n",
    "|n_splits_|交叉验证的折数|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- best_estimator_：训练完成后的最佳评估器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;实际上返回的就是带有网格搜索挑选出来的最佳参数（超参数）的评估器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, max_iter=1000000, penalty='l1', solver='saga')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述评估器就相当于一个包含最佳参数的逻辑回归评估器，可以调用逻辑回归评估器的所有属性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        , -3.47349066,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.55506614, -0.34227663,  3.03238721,  4.12147362]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看参数\n",
    "search.best_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9732142857142857, 0.9736842105263158)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看训练误差、测试误差\n",
    "search.best_estimator_.score(X_train,y_train), search.best_estimator_.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 1000000,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l1',\n",
       " 'random_state': None,\n",
       " 'solver': 'saga',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看参数\n",
    "search.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- best_score_：最优参数时交叉验证平均得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9644268774703558"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在默认情况下（未修改网格搜索评估器中评估指标参数时），此处的score就是准确率。此处有两点需要注意：      \n",
    "&emsp;&emsp;其一：该指标和训练集上整体准确率不同，该指标是交叉验证时验证集准确率的平均值，而不是所有数据的准确率；      \n",
    "&emsp;&emsp;其二：该指标是网格搜索在进行参数挑选时的参照依据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 其他属性方法测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.05980272, 0.02458634, 0.03755803, 0.02086544, 0.00778856,\n",
       "        0.012042  , 0.00317636, 0.00887041, 0.00049868, 0.00350785]),\n",
       " 'std_fit_time': array([5.92201677e-03, 1.05405387e-03, 1.34667070e-03, 5.44148798e-04,\n",
       "        3.86861563e-04, 2.92282252e-04, 1.52451976e-04, 1.55462472e-04,\n",
       "        3.12557937e-05, 6.50887088e-05]),\n",
       " 'mean_score_time': array([0.00021739, 0.00026002, 0.00019102, 0.00014124, 0.00024142,\n",
       "        0.00024619, 0.00019336, 0.0002326 , 0.00011868, 0.00018349]),\n",
       " 'std_score_time': array([5.97641527e-05, 1.38007910e-05, 7.29972748e-05, 5.19692566e-05,\n",
       "        2.59789181e-05, 1.48881665e-05, 3.58951028e-05, 1.17117652e-05,\n",
       "        1.74159836e-06, 4.23601048e-05]),\n",
       " 'param_C': masked_array(data=[1, 1, 0.5, 0.5, 0.1, 0.1, 0.05, 0.05, 0.01, 0.01],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 1, 'penalty': 'l1'},\n",
       "  {'C': 1, 'penalty': 'l2'},\n",
       "  {'C': 0.5, 'penalty': 'l1'},\n",
       "  {'C': 0.5, 'penalty': 'l2'},\n",
       "  {'C': 0.1, 'penalty': 'l1'},\n",
       "  {'C': 0.1, 'penalty': 'l2'},\n",
       "  {'C': 0.05, 'penalty': 'l1'},\n",
       "  {'C': 0.05, 'penalty': 'l2'},\n",
       "  {'C': 0.01, 'penalty': 'l1'},\n",
       "  {'C': 0.01, 'penalty': 'l2'}],\n",
       " 'split0_test_score': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.82608696, 1.        , 0.30434783, 0.91304348]),\n",
       " 'split1_test_score': array([0.91304348, 0.91304348, 0.82608696, 0.86956522, 0.82608696,\n",
       "        0.73913043, 0.69565217, 0.73913043, 0.39130435, 0.69565217]),\n",
       " 'split2_test_score': array([1.        , 1.        , 1.        , 1.        , 0.95454545,\n",
       "        0.95454545, 0.86363636, 0.90909091, 0.36363636, 0.86363636]),\n",
       " 'split3_test_score': array([0.95454545, 0.95454545, 0.95454545, 0.90909091, 0.95454545,\n",
       "        0.95454545, 0.86363636, 0.90909091, 0.36363636, 0.90909091]),\n",
       " 'split4_test_score': array([0.95454545, 0.95454545, 0.95454545, 0.95454545, 0.95454545,\n",
       "        0.90909091, 0.86363636, 0.95454545, 0.36363636, 0.90909091]),\n",
       " 'mean_test_score': array([0.96442688, 0.96442688, 0.94703557, 0.94664032, 0.93794466,\n",
       "        0.91146245, 0.82252964, 0.90237154, 0.35731225, 0.85810277]),\n",
       " 'std_test_score': array([0.03276105, 0.03276105, 0.06379941, 0.05120065, 0.05863407,\n",
       "        0.09083516, 0.06508431, 0.08830786, 0.02856808, 0.08323326]),\n",
       " 'rank_test_score': array([ 1,  1,  3,  4,  5,  6,  9,  7, 10,  8], dtype=int32)}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9732142857142857, 0.9736842105263158)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 等价于search.best_estimator_.score\n",
    "search.score(X_train,y_train), search.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.n_splits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07661604881286621"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.refit_time_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;至此，我们就执行了一个完整的网格搜索的调参过程。但该过程大多只使用了默认参数在小范围内进行的运算，如果我们希望更换模型评估指标、并且在一个更加完整的参数范围内进行搜索，则需要对上述过程进行修改，并更近一步掌握关于评估器中scoring参数和refit参数的相关使用方法，相关内容我们将在哦下一小节进行详细讨论。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
