{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6.6.1 多分类评估指标的macro与weighted过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在正式讨论关于网格搜索的进阶使用方法之前，我们需要先补充一些关于多分类问题的评估指标计算过程。在此前的课程中，我们曾经介绍过分类模型在解决多分类问题时的不同策略，同时也介绍过二分类问题的更高级评估指标，如f1-score和roc-auc等，接下来我们将详细讨论关于多分类预测结果在f1-socre和roc-auc中的评估过程，以及在sklearn中如何调用函数进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 科学计算模块\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 绘图模块\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 自定义模块\n",
    "from ML_basic_function import *\n",
    "\n",
    "# Scikit-Learn相关模块\n",
    "# 评估器类\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 实用函数\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 数据准备\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 多分类F1-Score评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先导入和F1-Score相关的评估指标计算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后简单查看相关说明文档，发现这几组和混淆矩阵相关的评估指标基本是共用了一套参数命名，并且大多数参数其实都是作用于多分类问题，对于二分类问题，我们可以简单调用相关函数直接计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 0, 0, 1, 0, 1])\n",
    "y_pred = np.array([1, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.75, 1.0, 0.8571428571428571)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_true, y_pred), recall_score(y_true, y_pred), f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'warn'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Compute the precision\n",
       "\n",
       "The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
       "true positives and ``fp`` the number of false positives. The precision is\n",
       "intuitively the ability of the classifier not to label as positive a sample\n",
       "that is negative.\n",
       "\n",
       "The best value is 1 and the worst value is 0.\n",
       "\n",
       "Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y_true : 1d array-like, or label indicator array / sparse matrix\n",
       "    Ground truth (correct) target values.\n",
       "\n",
       "y_pred : 1d array-like, or label indicator array / sparse matrix\n",
       "    Estimated targets as returned by a classifier.\n",
       "\n",
       "labels : list, optional\n",
       "    The set of labels to include when ``average != 'binary'``, and their\n",
       "    order if ``average is None``. Labels present in the data can be\n",
       "    excluded, for example to calculate a multiclass average ignoring a\n",
       "    majority negative class, while labels not present in the data will\n",
       "    result in 0 components in a macro average. For multilabel targets,\n",
       "    labels are column indices. By default, all labels in ``y_true`` and\n",
       "    ``y_pred`` are used in sorted order.\n",
       "\n",
       "    .. versionchanged:: 0.17\n",
       "       parameter *labels* improved for multiclass problem.\n",
       "\n",
       "pos_label : str or int, 1 by default\n",
       "    The class to report if ``average='binary'`` and the data is binary.\n",
       "    If the data are multiclass or multilabel, this will be ignored;\n",
       "    setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
       "    scores for that label only.\n",
       "\n",
       "average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
       "    This parameter is required for multiclass/multilabel targets.\n",
       "    If ``None``, the scores for each class are returned. Otherwise, this\n",
       "    determines the type of averaging performed on the data:\n",
       "\n",
       "    ``'binary'``:\n",
       "        Only report results for the class specified by ``pos_label``.\n",
       "        This is applicable only if targets (``y_{true,pred}``) are binary.\n",
       "    ``'micro'``:\n",
       "        Calculate metrics globally by counting the total true positives,\n",
       "        false negatives and false positives.\n",
       "    ``'macro'``:\n",
       "        Calculate metrics for each label, and find their unweighted\n",
       "        mean.  This does not take label imbalance into account.\n",
       "    ``'weighted'``:\n",
       "        Calculate metrics for each label, and find their average weighted\n",
       "        by support (the number of true instances for each label). This\n",
       "        alters 'macro' to account for label imbalance; it can result in an\n",
       "        F-score that is not between precision and recall.\n",
       "    ``'samples'``:\n",
       "        Calculate metrics for each instance, and find their average (only\n",
       "        meaningful for multilabel classification where this differs from\n",
       "        :func:`accuracy_score`).\n",
       "\n",
       "sample_weight : array-like of shape (n_samples,), default=None\n",
       "    Sample weights.\n",
       "\n",
       "zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
       "    Sets the value to return when there is a zero division. If set to\n",
       "    \"warn\", this acts as 0, but warnings are also raised.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "precision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
       "    Precision of the positive class in binary classification or weighted\n",
       "    average of the precision of each class for the multiclass task.\n",
       "\n",
       "See also\n",
       "--------\n",
       "precision_recall_fscore_support, multilabel_confusion_matrix\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.metrics import precision_score\n",
       ">>> y_true = [0, 1, 2, 0, 1, 2]\n",
       ">>> y_pred = [0, 2, 1, 0, 0, 1]\n",
       ">>> precision_score(y_true, y_pred, average='macro')\n",
       "0.22...\n",
       ">>> precision_score(y_true, y_pred, average='micro')\n",
       "0.33...\n",
       ">>> precision_score(y_true, y_pred, average='weighted')\n",
       "0.22...\n",
       ">>> precision_score(y_true, y_pred, average=None)\n",
       "array([0.66..., 0.        , 0.        ])\n",
       ">>> y_pred = [0, 0, 0, 0, 0, 0]\n",
       ">>> precision_score(y_true, y_pred, average=None)\n",
       "array([0.33..., 0.        , 0.        ])\n",
       ">>> precision_score(y_true, y_pred, average=None, zero_division=1)\n",
       "array([0.33..., 1.        , 1.        ])\n",
       "\n",
       "Notes\n",
       "-----\n",
       "When ``true positive + false positive == 0``, precision returns 0 and\n",
       "raises ``UndefinedMetricWarning``. This behavior can be\n",
       "modified with ``zero_division``.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体参数含义解释如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Name|Description|      \n",
    "|:--:|:--:| \n",
    "|y_true|数据集真实标签| \n",
    "|y_pred|标签预测结果|\n",
    "|labels|允许以列表形式输入其他形态的标签，一般不进行修改|\n",
    "|pos_label|positive类别标签|\n",
    "|average|多分类时指标计算方法|\n",
    "|sample_weight|不同类别的样本权重|\n",
    "|zero_division|当分母为0时返回结果|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，需要重点介绍多分类问题时average参数不同取值时的计算方法。此处以recall为例进行计算，重点介绍当average取值为'macro'、'micro'和'weighted'的情况，其他指标也类似，例如有简单多分类问题如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://tva1.sinaimg.cn/large/008i3skNly1gsgjd36uzbj30ry0pm0xi.jpg\" alt=\"1\" style=\"zoom:30%;\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们令1类标签为0、2类标签为1、3类标签为2，则上述数据集真实标签为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0, 1, 2, 2, 0, 1, 1, 2, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "并且最终分类预测结果为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([0, 1, 0, 2, 2, 1, 2, 2, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "据此我们可以构造多分类混淆矩阵如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://tva1.sinaimg.cn/large/008i3skNly1gsgjmmgd2dj314m0b676i.jpg\" alt=\"1\" style=\"zoom:30%;\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "据此我们可以计算三个类别的TP和FN："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp1 = 2\n",
    "tp2 = 2\n",
    "tp3 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn1 = 1\n",
    "fn2 = 1\n",
    "fn3 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来有两种计算recall的方法，其一是先计算每个类别的recall，然后求均值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "re1 = 2/3\n",
    "re2 = 2/3\n",
    "re3 = 3/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6944444444444443"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([re1, re2, re3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这也就是average参数取值为macro时的计算结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6944444444444443"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，如果上述手动实现过程不求均值，而是根据每个类别的数量进行加权求和，则就是参数average参数取值为weighted时的结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re1 * 3/10 + re2 * 3/10 + re3 * 4/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，还有另外一种计算方法，那就是先计算整体的TP和FN，然后根据整体TP和FN计算recall："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = tp1 + tp2 + tp3\n",
    "fn = fn1 + fn2 + fn3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp / (tp+fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该过程也就是average参数取值micro时的计算结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于上述三个不同参数的选取，首先如果是样本不平衡问题（如果是要侧重训练模型判别小类样本的能力的情况下）、则应排除weighted参数，以避免赋予大类样本更高的权重。除此以外，在大多数情况下这三个不同的参数其实并不会对最后评估器的选取结果造成太大影响，只是在很多要求严谨的场合下需要说明多分类的评估结果的计算过程，此时需要简单标注下是按照何种方法进行的计算。        \n",
    "&emsp;&emsp;不过，如果是混淆矩阵中相关指标和roc-auc指标放在一起讨论，由于新版sklearn中roc-auc本身不支持在多分类时按照micro计算、只支持macro计算，因此建议混淆矩阵的多分类计算过程也选择macro过程，以保持一致。后续在没有进行其他特殊说明的情况下，课上统一采用macro指标进行多分类问题评估指标的计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 不过值得注意的是，还有一种观点，尽管micro和macro方法在混淆矩阵相关指标的计算过程中差别不大，在roc-auc中，macro指标并不利于非平衡样本的计算（混淆矩阵中可以通过positive的类别选择来解决这一问题），需要配合ovr分类方法才能够有所改善。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 多分类ROC-AUC评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来继续讨论关于多分类的ROC-AUC评估指标的相关问题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够发现，roc_auc_score评估指标函数中大多数参数都和此前介绍的混淆矩阵中评估指标类似。接下来我们简单尝试使用roc-auc函数进行评估指标计算，根据roc-auc的计算流程可知，此处我们需要在y_pred参数位中输入模型概率预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 0, 0, 1, 0, 1])\n",
    "y_pred = np.array([0.9, 0.7, 0.2, 0.7, 0.4, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，如果我们在y_pred参数中输入分类结果，该函数也能计算出最终结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 0, 0, 1, 0, 1])\n",
    "y_pred = np.array([1, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过，此时模型会默认预测标签为0的概率结果为0.4、预测标签为1的概率预测结果为0.6，即上述结果等价于："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 0, 0, 1, 0, 1])\n",
    "y_pred = np.array([0.6, 0.6, 0.4, 0.6, 0.4, 0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 即计算过程会默认模型概率预测结果更差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来详细解释roc-auc中其他参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_fpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'raise'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n",
       "from prediction scores.\n",
       "\n",
       "Note: this implementation can be used with binary, multiclass and\n",
       "multilabel classification, but some restrictions apply (see Parameters).\n",
       "\n",
       "Read more in the :ref:`User Guide <roc_metrics>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y_true : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
       "    True labels or binary label indicators. The binary and multiclass cases\n",
       "    expect labels with shape (n_samples,) while the multilabel case expects\n",
       "    binary label indicators with shape (n_samples, n_classes).\n",
       "\n",
       "y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
       "    Target scores. In the binary and multilabel cases, these can be either\n",
       "    probability estimates or non-thresholded decision values (as returned\n",
       "    by `decision_function` on some classifiers). In the multiclass case,\n",
       "    these must be probability estimates which sum to 1. The binary\n",
       "    case expects a shape (n_samples,), and the scores must be the scores of\n",
       "    the class with the greater label. The multiclass and multilabel\n",
       "    cases expect a shape (n_samples, n_classes). In the multiclass case,\n",
       "    the order of the class scores must correspond to the order of\n",
       "    ``labels``, if provided, or else to the numerical or lexicographical\n",
       "    order of the labels in ``y_true``.\n",
       "\n",
       "average : {'micro', 'macro', 'samples', 'weighted'} or None,             default='macro'\n",
       "    If ``None``, the scores for each class are returned. Otherwise,\n",
       "    this determines the type of averaging performed on the data:\n",
       "    Note: multiclass ROC AUC currently only handles the 'macro' and\n",
       "    'weighted' averages.\n",
       "\n",
       "    ``'micro'``:\n",
       "        Calculate metrics globally by considering each element of the label\n",
       "        indicator matrix as a label.\n",
       "    ``'macro'``:\n",
       "        Calculate metrics for each label, and find their unweighted\n",
       "        mean.  This does not take label imbalance into account.\n",
       "    ``'weighted'``:\n",
       "        Calculate metrics for each label, and find their average, weighted\n",
       "        by support (the number of true instances for each label).\n",
       "    ``'samples'``:\n",
       "        Calculate metrics for each instance, and find their average.\n",
       "\n",
       "    Will be ignored when ``y_true`` is binary.\n",
       "\n",
       "sample_weight : array-like of shape (n_samples,), default=None\n",
       "    Sample weights.\n",
       "\n",
       "max_fpr : float > 0 and <= 1, default=None\n",
       "    If not ``None``, the standardized partial AUC [2]_ over the range\n",
       "    [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,\n",
       "    should be either equal to ``None`` or ``1.0`` as AUC ROC partial\n",
       "    computation currently is not supported for multiclass.\n",
       "\n",
       "multi_class : {'raise', 'ovr', 'ovo'}, default='raise'\n",
       "    Multiclass only. Determines the type of configuration to use. The\n",
       "    default value raises an error, so either ``'ovr'`` or ``'ovo'`` must be\n",
       "    passed explicitly.\n",
       "\n",
       "    ``'ovr'``:\n",
       "        Computes the AUC of each class against the rest [3]_ [4]_. This\n",
       "        treats the multiclass case in the same way as the multilabel case.\n",
       "        Sensitive to class imbalance even when ``average == 'macro'``,\n",
       "        because class imbalance affects the composition of each of the\n",
       "        'rest' groupings.\n",
       "    ``'ovo'``:\n",
       "        Computes the average AUC of all possible pairwise combinations of\n",
       "        classes [5]_. Insensitive to class imbalance when\n",
       "        ``average == 'macro'``.\n",
       "\n",
       "labels : array-like of shape (n_classes,), default=None\n",
       "    Multiclass only. List of labels that index the classes in ``y_score``.\n",
       "    If ``None``, the numerical or lexicographical order of the labels in\n",
       "    ``y_true`` is used.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "auc : float\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] `Wikipedia entry for the Receiver operating characteristic\n",
       "        <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
       "\n",
       ".. [2] `Analyzing a portion of the ROC curve. McClish, 1989\n",
       "        <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_\n",
       "\n",
       ".. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving\n",
       "       probability estimation trees (Section 6.2), CeDER Working Paper\n",
       "       #IS-00-04, Stern School of Business, New York University.\n",
       "\n",
       ".. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern\n",
       "        Recognition Letters, 27(8), 861-874.\n",
       "        <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_\n",
       "\n",
       ".. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area\n",
       "        Under the ROC Curve for Multiple Class Classification Problems.\n",
       "        Machine Learning, 45(2), 171-186.\n",
       "        <http://link.springer.com/article/10.1023/A:1010920819831>`_\n",
       "\n",
       "See also\n",
       "--------\n",
       "average_precision_score : Area under the precision-recall curve\n",
       "\n",
       "roc_curve : Compute Receiver operating characteristic (ROC) curve\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.metrics import roc_auc_score\n",
       ">>> y_true = np.array([0, 0, 1, 1])\n",
       ">>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
       ">>> roc_auc_score(y_true, y_scores)\n",
       "0.75\n",
       "\u001b[0;31mFile:\u001b[0m      ~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roc_auc_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Name|Description|      \n",
    "|:--:|:--:| \n",
    "|max_fpr|fpr最大值，fpr是roc曲线的横坐标| \n",
    "|multi_class|分类器在进行多分类时进行的多分类问题处理策略|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此处需要注意的是关于multi_class参数的选择。一般来说sklearn中的multi_class参数都是二分类器中用于解决多元分类问题时的参数（如逻辑回归），而由于roc-auc需要分类结果中的概率来完成最终计算，因此需要知道概率结果对应分类标签——即到底是以ovo还是ovr模式在进行多分类，因此如果是进行多分类roc-auc计算时，需要对其进行明确说明。       \n",
    "&emsp;&emsp;不过对于多分类逻辑回归来说，无论是ovr还是mvm策略，最终分类结果其实都可以看成是ovr分类结果，因此如果是多分类逻辑回归计算roc-auc，需要设置multi_class参数为ovr。同时由于根据roc-auc的函数参数说明可知，在multi_class参数取为ovr时，average参数取值为macro时能够保持一个较高的偏态样本敏感性，因此对于roc-auc来说，大多数时候average参数建议取值为macro。总结一下，对于roc-auc进行多分类问题评估时，建议选择的参数组合是ovr/ovo+macro，而ovr/ovo的参数选择需要根据具体的多分类模型来定，如果是围绕逻辑回归多分类评估器来进行结果评估，则建议roc-auc和逻辑回归评估器的multi_class参数都选择ovr。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在新版的sklearn中，roc-auc函数的multi_class参数已不支持micro参数，面对多分类问题，该参数只能够在macro和weighted中进行选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们简单测算average参数中macro和weighted的计算过程。还是围绕上述数据集进行计算："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://tva1.sinaimg.cn/large/008i3skNly1gsgjd36uzbj30ry0pm0xi.jpg\" alt=\"1\" style=\"zoom:30%;\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "据此我们可以计算每个类别单独的roc-auc值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_1 = np.array([1, 0, 0, 0, 1, 0, 0, 0, 1, 0])\n",
    "y_pred_1 = np.array([0.8, 0.2, 0.5, 0.2, 0.3, 0.1, 0.3, 0.3, 0.9, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8809523809523809"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = roc_auc_score(y_true_1, y_pred_1)\n",
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_2 = np.array([0, 1, 0, 0, 0, 1, 1, 0, 0, 0])\n",
    "y_pred_2 = np.array([0.2, 0.6, 0.3, 0, 0.2, 0.8, 0.2, 0.3, 0, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = roc_auc_score(y_true_2, y_pred_2)\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_3 = np.array([0, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n",
    "y_pred_3 = np.array([0, 0.2, 0.2, 0.8, 0.5, 0.1, 0.5, 0.4, 0.1, 0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3 = roc_auc_score(y_true_3, y_pred_3)\n",
    "r3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时r1、r2、r3的均值如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8501984126984127"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([r1, r2, r3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该结果应当和macro+multi_class参数计算结果相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8, 0.2, 0. ],\n",
       "       [0.2, 0.6, 0.2],\n",
       "       [0.5, 0.3, 0.2],\n",
       "       [0.2, 0. , 0.8],\n",
       "       [0.3, 0.2, 0.5],\n",
       "       [0.1, 0.8, 0.1],\n",
       "       [0.3, 0.2, 0.5],\n",
       "       [0.3, 0.3, 0.4],\n",
       "       [0.9, 0. , 0.1],\n",
       "       [0.3, 0.1, 0.6]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.concatenate([y_pred_1.reshape(-1, 1), y_pred_2.reshape(-1, 1), y_pred_3.reshape(-1, 1)], 1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0, 1, 2, 2, 0, 1, 1, 2, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8501984126984127"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，如果roc-auc函数的参数是ovr+weighted，则计算结果过程验证如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8464285714285713"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 * 3/10 + r2 * 3/10 + r3 * 4/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8464285714285713"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_true, y_pred, average='weighted', multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们就能够较为清楚的了解关于f1-score和roc-auc评估指标在调用sklearn中相关函数解决多分类问题评估的具体方法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
